{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlFqklLLT61y7Td19HWyVh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucas-pinto/NUIN443/blob/main/problemSets/NUIN443_ps8_dimensionality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Packages**"
      ],
      "metadata": {
        "id": "-iximm4Hj6Ah"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEioygwHj1Qk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Some helper functions for Gaussians\n",
        "from numpy.random import normal, multivariate_normal\n",
        "from scipy.stats import norm\n",
        "\n",
        "#orthogonalization\n",
        "from scipy.linalg import orth\n",
        "\n",
        "#Sci-kit linear factor analysis and PCA, Linear Regression, and IsoMap\n",
        "from sklearn.decomposition import FactorAnalysis, PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.manifold import Isomap\n",
        "\n",
        "#r2 metric\n",
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Holding out test-set data during dimensionality reduction\n",
        "In this problem, we'll explore why have a held-out test set that includes both held-out neurons and held-out timepoints is important. In particular, we'll demonstrate how the r2 values will keep increasing beyond the true dimensionality if you only hold out neurons or timepoints)\n",
        "\n",
        "Here, we won't be doing full cross-validation (for the sake of simplicity), but will just be doing a train/test split."
      ],
      "metadata": {
        "id": "AeOZO4vVk-Ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) We'll first simulate data generated from a 5-dimensional latent space (with random values), which is projected up to a 50-dimensional observation space through a random orthogoal matrix. Noise is added in the 50-dimensional space. The final neural data is \"X\", which is size timepoints x neurons. <br>\n",
        "Just run the code below."
      ],
      "metadata": {
        "id": "uVL8qog9ljzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the random seed so we get the same results every time\n",
        "np.random.seed(0)\n",
        "\n",
        "T=2000 #number of timepoints\n",
        "num_latents=5 #number of latents\n",
        "num_observations=50 #number of neurons (observations)\n",
        "\n",
        "Z=np.random.randn(T,num_latents) #Random latents\n",
        "\n",
        "U=orth(np.random.randn(num_observations,num_latents)).T #Random projection matrix\n",
        "\n",
        "X=Z@U+1*np.random.randn(T,num_observations) #High-dimensional \"neural\" data"
      ],
      "metadata": {
        "id": "zPZd95oklgqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) Just split training/testing based on time (not neurons):\n",
        "\n",
        "*   Make the first half of time points the training and the second half testing\n",
        "*   Fit PCA on the training data\n",
        "*   Evaluate the fit on the testing data using \"r2_score\" (imported above)\n",
        "*   Do this for all possible dimensionalities (1-50)\n",
        "*   As the output at the end, create a plot of the r2 values as a function of the dimensionality.\n",
        "\n",
        "We have provided some code that you can fill in to get started.\n"
      ],
      "metadata": {
        "id": "d2Q3V_ExZfqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fill in below\n",
        "X_train=\n",
        "X_test=\n",
        "\n",
        "r2s=[] #Initialize list of r2 values\n",
        "\n",
        "#Loop through dimensions. Fill in below\n",
        "for k in\n",
        "\n",
        "  #Fit PCA to training data. Fill in lines below\n",
        "\n",
        "\n",
        "  #Reconstruct test set data using the PCA model fit above. Rename \"pca\" below if your pca model has a different name above\n",
        "  X_test_hat=(X_test-pca.mean_[None,:])@pca.components_.T@pca.components_+pca.mean_[None,:]\n",
        "\n",
        "  #Calculate r2 value and append it to list. Fill in below\n",
        "  r2s.append()\n",
        "\n",
        "plt.plot(np.arange(1,num_observations+1),r2s)"
      ],
      "metadata": {
        "id": "94o4Ht3-R5Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C) Just split training/testing based on neurons (not time):\n",
        "\n",
        "*   Make the first half of neurons the training set and the second half testing\n",
        "*   Fit PCA on the training neurons\n",
        "*   Use linear regression to predict the test-set neurons from the PCs you just fit (based on the training neurons)\n",
        "*   Evaluate the fit\n",
        "*   Do this for all possible dimensionalities (1-25). Note that the top dimensionality you can test is 25, since we are splitting the neurons in half.\n",
        "*   As the output at the end, create a plot of the r2 values as a function of the dimensionality.\n",
        "\n",
        "We have provided some code that you can fill in to get started.\n"
      ],
      "metadata": {
        "id": "8eETI-tZZmXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fill in below\n",
        "X_train=\n",
        "X_test=\n",
        "\n",
        "r2s=[] #Initialize list of r2 values\n",
        "\n",
        "#Loop through dimensions. Fill in below\n",
        "for k in\n",
        "\n",
        "  #Fit PCA to training neurons and get PCs. Fill in lines below\n",
        "\n",
        "\n",
        "  #Fit linear regression to predict test-set neurons from the above PCs. Fill in lines below\n",
        "\n",
        "\n",
        "  #Calculate r2 value and append it to list. Fill in below\n",
        "  r2s.append()\n",
        "\n",
        "\n",
        "#Plot r2 values\n",
        "plt.plot(np.arange(1,int(num_observations/2+1)),r2s)"
      ],
      "metadata": {
        "id": "qNX0E7f7ZlU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "D) Split training/testing based on neurons and time:\n",
        "\n",
        "\n",
        "*   First split the data into train/test based on neurons (as you did in C).\n",
        "*   Fit PCA on the training neurons\n",
        "*   Train a linear regression model to predict the test-set neurons from the PCs you just fit, *but only using the first half of time points (training time points)*\n",
        "*   Evaluate the fit of this linear regression model on the testing time points\n",
        "*   Do this for all possible dimensionalities (1-25). Note that the top dimensionality you can test is 25, since we are splitting the neurons in half.\n",
        "*   As the output at the end, create a plot of the r2 values as a function of the dimensionality."
      ],
      "metadata": {
        "id": "vtyACv72cbUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fill in all the code below (you should be able to use a lot of code from C)"
      ],
      "metadata": {
        "id": "jn7qH4t7c8zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that estimating the exact dimensionality will generally be improved with cross-validation than with a single train/test split (above it underestimates the dimensionality as 4, but when the random seed is seed to 1 it overestimates the dimensionality)"
      ],
      "metadata": {
        "id": "ARmZAZEvVbhX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Nonlinear and linear dimensionality reduction"
      ],
      "metadata": {
        "id": "PGL_KPi7uJY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an overly simplistic simulation of place cell activity as an animal runs along a 1 dimensional trajectory at a constant pace. <br>\n",
        "Each neuron has a place cell with a Gaussian tuning curve centered at a different place along the track.\n",
        "<br>\n",
        "Let's see how linear and nonlinear dimensionality reduction methods compare"
      ],
      "metadata": {
        "id": "0l5WJxz4P1YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of neurons recorded\n",
        "num_nrns=20\n",
        "\n",
        "#Locations that the animal runs at each time step\n",
        "xs=np.arange(start=0,stop=21,step=.01)\n",
        "\n",
        "#Centers of all the tuning curves\n",
        "mus=np.arange(1,num_nrns+1)\n",
        "#Standard deviation of the tuning curves\n",
        "sigma=1\n",
        "\n",
        "#Generate the neural activity for each neuron, and format it to be in a matrix of size timepoints (samples) x neurons\n",
        "X=np.vstack([norm.pdf(xs,mus[i],sigma) for i in range(20)]).T"
      ],
      "metadata": {
        "id": "bAnaO3Xu5tjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run below to plot the tuning curves of some example neurons. <br>\n",
        "Note that the plot both shows tuning curves as a function of location, and the neuron's activities as a function of time (since in this simulation we're assuming the animal is moving along this track at a constant speed)"
      ],
      "metadata": {
        "id": "ac7mHsXkexu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(X[:,0])\n",
        "plt.plot(X[:,5])\n",
        "plt.plot(X[:,10])\n",
        "plt.plot(X[:,15])"
      ],
      "metadata": {
        "id": "emMK-2uVeX5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) Run PCA on the above data with full dimensionality (20)."
      ],
      "metadata": {
        "id": "70OT2J50fkko"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lnR07O1QCjBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) Plot the explained variance ratio, to get an intuition for the linear dimensionality of the data (i.e., that it seems high dimensional)."
      ],
      "metadata": {
        "id": "LtU7Wekvf5Al"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qeo73_7uf-XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C) Plot the first 3 PCs as a function of time."
      ],
      "metadata": {
        "id": "zCBJ9phUg_1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Hviwd2xEciJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "D) The below code will run Isomap, a nonlinear dimensionality reduction technique, on the data, with 2 dimensions. After running, plot the two dimensions of the data."
      ],
      "metadata": {
        "id": "7nJVSNHMhKVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "isomap=Isomap(n_components=2)\n",
        "z_iso=isomap.fit_transform(X)"
      ],
      "metadata": {
        "id": "GobA1h3CCFpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the latents"
      ],
      "metadata": {
        "id": "r2uN0KndEHXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E) There is not a direct equivalent to the explained variance of the high-dimensional data for Isomap (it projects the data into a lowD space, but there is not a learned mapping back to the highD space). People do sometimes determine the variance in the lowD space, but that's not readily available as part of Isomap in scikit learn, so we won't be doing that :)\n",
        "<br>\n",
        "That being said, in the text box below, write the apparent nonlinear dimensionality of the system based on the above plot."
      ],
      "metadata": {
        "id": "OuS8ALAwin_h"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xhmfYg7VEHbf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}