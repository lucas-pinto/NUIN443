{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4T5I6Rw6q+v9xs5KShFb+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucas-pinto/NUIN443/blob/main/problemSets/NUIN443_ps5_FA%2BGPs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Packages**"
      ],
      "metadata": {
        "id": "-iximm4Hj6Ah"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZEioygwHj1Qk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Some helper functions for Gaussians\n",
        "from numpy.random import normal, multivariate_normal\n",
        "\n",
        "#Sci-kit linear factor analysis and PCA\n",
        "from sklearn.decomposition import FactorAnalysis, PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Factor Analysis\n",
        "\n",
        "1) Write code to simulate data from the  factor analysis model with the following parameters, with 1 latent and 3-dimensional observations:  \n",
        "\n",
        "$W = \\begin{bmatrix} -1 \\\\ 1 \\\\ 2 \\end{bmatrix}$\n",
        "$\\Psi = \\begin{bmatrix} 10 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$\n",
        "\n",
        "<br>\n",
        "\n",
        "*Credit: This problem is inspired from a problem from Jonathan Pillow's Computational Neuroscience course at Princeton*"
      ],
      "metadata": {
        "id": "AeOZO4vVk-Ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) First, generate 2000 samples of 1-dimensional latent $z$"
      ],
      "metadata": {
        "id": "uVL8qog9ljzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the random seed so we get the same results every time\n",
        "np.random.seed(1)\n",
        "\n",
        "#Fill in below to generate samples of the latent\n",
        "\n"
      ],
      "metadata": {
        "id": "zPZd95oklgqo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change below code if necessary, based on the variable name you gave to Z\n",
        "\n",
        "# Sort samples of Z in order to make visualization of the latent more obvious later on\n",
        "Z=np.sort(Z)"
      ],
      "metadata": {
        "id": "Hbi8Mez7JTPH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) From the latent, generate 3-dimensional samples $x$. <br> Also, create a version, $x_{noiseless}$, that doesn't include the observation noise ($\\Psi$)."
      ],
      "metadata": {
        "id": "-i78o4OEleF2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8rC3hWNmk5Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Make a scatter plot showing the first two dimensions of $x$ samples.\n",
        "<br> Overlay the same for $x_{noiseless}$, to get an intuition for how the shared signal and noise differ.\n",
        "<br> Make the x and y axes have equal limits"
      ],
      "metadata": {
        "id": "qn_Gp6MgnNKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fill in below to generate a scatter plot of the first two dimensions of x samples\n",
        "plt.scatter()\n",
        "\n",
        "#Fill in below to generate a scatter plot of the first two dimensions of x_noiseless samples\n",
        "plt.scatter()\n",
        "\n",
        "plt.ylim([-12,12])\n",
        "plt.xlim([-12,12])"
      ],
      "metadata": {
        "id": "c9vx2b9Jq5_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d) To make this more clearly connected to neuroscience, let's assume each of these samples was taken over the course of 2000 time points. Plot the latent and the first two dimensions of $x$ (the 'activity of two neurons') by filling in the below code"
      ],
      "metadata": {
        "id": "qsPYPX6IsEZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fill in below to plot latent\n",
        "plt.subplot(3,1,1)\n",
        "plt.plot()\n",
        "\n",
        "#Fill in below to plot first dimension of x\n",
        "plt.subplot(3,1,2)\n",
        "plt.plot()\n",
        "\n",
        "#Fill in below to plot second dimension of x\n",
        "plt.subplot(3,1,3)\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "tS5HUOISojZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e) Fit a factor analysis model with 1 latent to the data, using sci-kit learn, and get the fit latent (we'll be plotting this latent later). Note that the package, FactorAnalysis, has already been imported"
      ],
      "metadata": {
        "id": "8_v70B2quRX8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kYYdnPd8uPlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "f) Print the model's loadings (components_) and noise (noise_variance_) to check that they're approximately the same as the model you generated the data from."
      ],
      "metadata": {
        "id": "kDjHZVB0UYj_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K4m560rlusFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UPr3fOUgu38P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "g) Fit a PCA model to the data. You can also use sci-kit learn. Print the loadings (components_) to see how this differs from factor analysis"
      ],
      "metadata": {
        "id": "jVxuU2NKUswG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UXYTQeV67X0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "h) Replot the scatter plot from (c). We will now overlay a line with the FA loading axis and a line with the PCA loading axis (just the first two dimensions of those)."
      ],
      "metadata": {
        "id": "fvU94DiNVcDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fill in below to generate a scatter plot of the first two dimensions of x samples\n",
        "plt.scatter()\n",
        "\n",
        "#Fill in below to generate a scatter plot of the first two dimensions of x_noiseless samples\n",
        "plt.scatter()\n",
        "\n",
        "#This will plot the axes of the PCA and FA components_ (loading axes)\n",
        "plt.plot([-10*pca.components_[0][0],10*pca.components_[0][0]],[-10*pca.components_[0][1],10*pca.components_[0][1]],'r--',linewidth=3)\n",
        "plt.plot([-10*fa.components_[0][0],10*fa.components_[0][0]],[-10*fa.components_[0][1],10*fa.components_[0][1]],'k--',linewidth=3)\n",
        "\n",
        "plt.ylim([-12,12])\n",
        "plt.xlim([-12,12])\n",
        "\n",
        "#This will add a legend\n",
        "plt.legend(['X','X_noiseless','PCA axis','FA axis'])"
      ],
      "metadata": {
        "id": "e_dWg5vDZyAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "i) Explain in words, in the cell below, why the loadings for PCA and FA are different"
      ],
      "metadata": {
        "id": "WovOe3K6KZ-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vMuYf4XE1bJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "j) Plot the ground truth latent, and the recovered latent via FA, overlaid"
      ],
      "metadata": {
        "id": "V_L_FfQjhDBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fill in below to plot the latent recovered from factor analysis\n",
        "plt.plot()\n",
        "\n",
        "#Fill in below to plot the ground truth latent\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "pBxCzavhhcJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "k) Plot the ground truth latent and the recovered latent via PCA, overlaid"
      ],
      "metadata": {
        "id": "5UjbSpN9iNon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fill in below to plot the latent recovered from PCA\n",
        "#Note that, by chance, the PCA axes in this example are flipped relative to the ground truth\n",
        "#(this is a model degeneracy, where both the latents and axes can be flipped and we get an equal result)\n",
        "#Therefore, actually plot the negative of the recovered latent from PCA below for a better match to the ground truth\n",
        "plt.plot()\n",
        "\n",
        "#Fill in below to plot the ground truth latent\n",
        "plt.plot()"
      ],
      "metadata": {
        "id": "9uVxvqwlhqwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L) Explain in words, in the cell below, why PCA's estimate of the latent are so inaccurate relative to FA. In particular, consider which dimensions are being most heavily utilized to estimate the latent."
      ],
      "metadata": {
        "id": "qgMVOawKpY_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CTu1ovUWpf2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "m) While you were able to use sci-kit learn for factor analysis rather than fully implementing EM, let's just do the \"E\" step here using the final model parameters found with sci-kit learn (fa.noise_variance_ and fa.components_). That is, find p(z|x), using the equation shown in class. You can just find the mean here and not worry about the variance.<br>\n",
        "\n",
        "Then, create a scatter plot of the latent estimated manually, versus the latent directly output from sci-kit learn. If the above calculations were correct, this should be a diagonal line."
      ],
      "metadata": {
        "id": "PmsDYJr8kbEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fill in code below to find p(z|x)\n",
        "\n",
        "\n",
        "#Fill in code below to create a scatter plot of the latent estimated above, versus the latent output from sci-kit learn (from part e)\n",
        "plt.scatter()"
      ],
      "metadata": {
        "id": "cvhcwFeHpicS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zczppAFqQJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "n) The above showed pretty large differences between factor analysis and PCA to demonstrate how they differ, but for most neural datasets, the difference is more minor. <br><br> Re-run the above with FA vs PCA comparison on data generated with the following, less extreme, noise model:\n",
        "$\\Psi = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$\n",
        "<br>\n",
        "Just output the same plot as in (h)\n",
        "\n",
        "And by re-run, I mean copy-paste your code from a,b,e,g,h (while changing $\\Psi$) into your cell below, so you don't overwrite your previous results."
      ],
      "metadata": {
        "id": "6PXz26xTb5xk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cx6peOd7N5hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Gaussian Processes"
      ],
      "metadata": {
        "id": "PGL_KPi7uJY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have provided a function for a radial basis function kernel below"
      ],
      "metadata": {
        "id": "0l5WJxz4P1YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#In the below function, x is a vector of datapoints, and L (the lengthscale) is a scalar\n",
        "\n",
        "def cov_RBF(x,L):\n",
        "  cov_rbf = np.exp(-(x-x.T)**2/(2*L**2))\n",
        "  return cov_rbf"
      ],
      "metadata": {
        "id": "bAnaO3Xu5tjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Let's say our datapoints have values 1,2,...,399,400 (e.g. these are the values of timepoints).  Plot (imshow) the RBF covariance for those values, for lengthscale=100 and lengthscale=1. Fill in the missing code below to do so."
      ],
      "metadata": {
        "id": "amM1jcWRzMPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Datapoints X\n",
        "X = np.arange(1,400)[None,:]\n",
        "\n",
        "#Fill in below to generate the RBF kernel for lengthscale=100\n",
        "scale1=\n",
        "K_rbf1 = cov_RBF()\n",
        "\n",
        "#Plot the image\n",
        "plt.figure()\n",
        "plt.imshow(K_rbf1,clim=[0,1])\n",
        "plt.title('Lengthscale 100')\n",
        "plt.colorbar()\n",
        "\n",
        "#Fill in below to generate the RBF kernel for lengthscale=1\n",
        "scale2=\n",
        "K_rbf2 = cov_RBF()\n",
        "\n",
        "#Plot the image\n",
        "plt.figure()\n",
        "plt.imshow(K_rbf2,clim=[0,1])\n",
        "plt.title('Lengthscale 1')\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "8jOKXsJX9Roa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) For both of the above RBF covariance functions, sample datapoints from the multivariate normal distribution with mean 0 and those covariances. Plot both of these samples. Note that the result will be 400 datapoints. As a hint, the means that you input into the multivariate_normal function will need to be arrays with 400 zeros."
      ],
      "metadata": {
        "id": "HPK4hEoI0Gu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fill in below to generate mean of distributions\n",
        "mu=\n",
        "\n",
        "#Fill in below to generate samples from the above mean and the covariance of an RBF kernel with lengthscale 100 (from above)\n",
        "sample1 = np.random.multivariate_normal(mean=, cov=)\n",
        "#Fill in below to generate samples from the above mean and the covariance of an RBF kernel with lengthscale 1 (from above)\n",
        "sample2 = np.random.multivariate_normal(mean=, cov=)\n",
        "\n",
        "#Plot the samples\n",
        "plt.figure()\n",
        "plt.plot(sample1)\n",
        "plt.title('GP Sample for lengthscale=100')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(sample2)\n",
        "plt.title('GP Sample for lengthscale=1')"
      ],
      "metadata": {
        "id": "Gf8UfzwldIYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain in words how the differences in the covariance functions in (a) (for different scale parameters) explain the differences in the samples from the distributions (b)"
      ],
      "metadata": {
        "id": "lsduY-Uv1s9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ADI6d-MG2VzJ"
      }
    }
  ]
}