{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucas-pinto/NUIN443/blob/main/problemSets/ps12_RNN_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b661d1a",
      "metadata": {
        "id": "1b661d1a"
      },
      "outputs": [],
      "source": [
        "! pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9580f35",
      "metadata": {
        "id": "a9580f35"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9630fbcf",
      "metadata": {
        "id": "9630fbcf"
      },
      "source": [
        "## Problem Set 12: Computing with Recurrent Neural Networks\n",
        "Our goal today is to simulate a firing rate recurrent neural network, and develop an intuition for how it behaves.\n",
        "\n",
        "Let's start by implementing a standard RNN of $N$ neurons with a hyperbolic tangent nonlinearity. Written in matrix notation, the dynamics of this system are defined by the equation:\n",
        "\n",
        "\\begin{equation}\n",
        "\\tau \\frac{dx}{dt} = -x + J\\phi(x) + J^{in}u\\\\\n",
        "\\end{equation}\n",
        "\n",
        "where $\\phi(x) = \\tanh(x)$ is our firing rate nonlinearity, $J$ is our $N\\times N$ recurrent weight matrix where $J_{ij} \\sim \\mathcal{N}(0,\\frac{g}{\\sqrt{N}})$, $\\tau$ is the membrane time constant, $u$ is time-varying external input, and $J^{in}$ is a set of weights by which that input drives our neurons, where we will take $J^{in}_{ij} \\sim \\mathcal{N}(0,w)$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20f45b51",
      "metadata": {
        "id": "20f45b51"
      },
      "source": [
        "### Problem 1. Simluating a random RNN.\n",
        "First, let's simulate a randomly connected RNN and see how it responds to a driving input.\n",
        "\n",
        "<font color=violet>1a. Complete the code snippet below to simulate the dynamics of a randomly connected RNN with recurrent weight matrix $J$ and input weights $J^{in}$ defined as above, and network parameters defined by the entries of the params struct.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec77fc01",
      "metadata": {
        "id": "ec77fc01"
      },
      "outputs": [],
      "source": [
        "params = {'N': 1000,      # number of neurons in the network\n",
        "          'tau': 10,      # \"membrane\" time constant\n",
        "          'dt': 1,        # simulation timestep\n",
        "          'g': 2,         # gain on recurrent weights\n",
        "          'n_inputs': 2,  # number of input channels\n",
        "          'w': 0.1}       # variance of input weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3435f54a",
      "metadata": {
        "scrolled": false,
        "id": "3435f54a"
      },
      "outputs": [],
      "source": [
        "def simulate_RNN(params, u = np.zeros(1), T=5000, J = np.zeros(1)):\n",
        "\n",
        "    N = params['N']\n",
        "    tau = params['tau']\n",
        "    dt = params['dt']\n",
        "    g = params['g']\n",
        "    n_inputs = params['n_inputs']\n",
        "\n",
        "    if not J.any():\n",
        "        J = ...\n",
        "\n",
        "    W = ...\n",
        "\n",
        "    if not u.any():\n",
        "        u = np.zeros((n_inputs,T))\n",
        "    else:\n",
        "        T = u.shape[1]\n",
        "\n",
        "    x = np.random.rand(N,1)\n",
        "    rstore = np.zeros((T,N))\n",
        "\n",
        "    for t in range(1,T):\n",
        "        dxdt = ...\n",
        "        rstore[t,:] = ...\n",
        "\n",
        "    return rstore"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc34a373",
      "metadata": {
        "id": "cc34a373"
      },
      "source": [
        "<font color=violet>1b. To check your work, simulate the network for 5000 timesteps in two conditions: first, simulate the spontaneous dynamics of the network; second, simulate the dynamics in response to a 2d driving input of $\\sin(2\\pi t * 0.001)$ and $\\cos(2\\pi t*0.001)$. Pick 5 units from the network and plot their activity and their autocorrelation for 2000 lags, for both cases. In the case of the driven network, omit the first 1000 timesteps from your calculation of autocorrelation to allow some \"burn-in\" time.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17074220",
      "metadata": {
        "id": "17074220"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "30a65b6d",
      "metadata": {
        "id": "30a65b6d"
      },
      "source": [
        "<font color=violet>1c. Now, let's look at how network dynamics change as a function of input amplitude. For your same network, plot the average autocorrelation across 50 random neurons in the model (again for 2000 lags) for a range of input magnitudes, until you see evidence of your input in your network's average autocorrelation. At what input magnitude does the network become entrained? Repeat using g=0.5 and g=5; what changes?</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9401571d",
      "metadata": {
        "id": "9401571d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3a90ffb0",
      "metadata": {
        "id": "3a90ffb0"
      },
      "source": [
        "### Problem 2. Training an RNN using Torch\n",
        "\n",
        "In practice, you're likely going to find yourself using a package like PyTorch or Tensorflow for simulating or training RNNs. While we're not going to make you code up this training process from scratch, it's worth running the steps yourself to see what you can do with the results. We'll provide the model setup and training code to work from.\n",
        "\n",
        "To start, let's define a task: we're going to drive our network with a pair of inputs:\n",
        "\\begin{equation}\n",
        "u_1(t) = \\sin(2\\pi f_0 t + \\phi) + \\sin(2\\pi\\frac{f_0}{2} t + \\phi_{d}) + \\eta_1(t)\\\\\n",
        "u_2(t) = \\cos(2\\pi f_0 t + \\phi) + \\cos(2\\pi\\frac{f_0}{2} t + \\phi_{d}) + \\eta_2(t)\n",
        "\\end{equation}\n",
        "\n",
        "where $\\phi$ and $\\phi_d$ are randomly selected phase shifts and $\\eta(t)$ is Gaussian noise. We want our network to predict the value of the first sin and cosine signals 5 timesteps into the future, and ignore the other terms.\n",
        "\n",
        "\\begin{equation}\n",
        "f_1(t) = \\sin(2\\pi f_0 (t+5) + \\phi)\\\\\n",
        "f_2(t) = \\cos(2\\pi f_0 (t+5) + \\phi)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2608c956",
      "metadata": {
        "id": "2608c956"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_sine_cosine_data(seq_length, num_sequences, freq=10, time_shift=5):\n",
        "    X = []\n",
        "    y = []\n",
        "    for _ in range(num_sequences):\n",
        "        t = np.linspace(0, seq_length, num=seq_length)\n",
        "\n",
        "        # create our target signal- a sine wave and a cosine wave at the provided frequency, starting with random phase.\n",
        "        phi = np.random.rand(1) * freq\n",
        "        sine_target = np.sin(2 * np.pi * freq * t + phi)\n",
        "        cosine_target = np.cos(2 * np.pi * freq * t + phi)\n",
        "        sequence_target = np.stack((sine_target, cosine_target), axis=1)\n",
        "\n",
        "        # create some distractor inputs to the network.\n",
        "        phi = np.random.rand(1) * freq\n",
        "        sine_distractor = np.sin(2 * np.pi * freq/2 * t + phi)\n",
        "        cosine_distractor = np.cos(2 * np.pi * freq/2 * t + phi)\n",
        "        noise_term = np.random.randn(sequence_target.shape[0],sequence_target.shape[1])\n",
        "\n",
        "        sequence_input = np.stack((sine_target + sine_distractor, cosine_target + cosine_distractor), axis=1) + noise_term\n",
        "\n",
        "        X.append(sequence_input[:-time_shift])\n",
        "        y.append(sequence_target[time_shift:])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 300\n",
        "num_sequences = 100\n",
        "X, y = generate_sine_cosine_data(seq_length, num_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d053f056",
      "metadata": {
        "id": "d053f056"
      },
      "source": [
        "The following code will initialize our RNN model; be sure you run it and the cell above to set things up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bcb257f",
      "metadata": {
        "id": "7bcb257f"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # optional: if you uncomment the following lines you can modify the initial\n",
        "        # weights of the model to see how it affects learning:\n",
        "#         weights = self.rnn.state_dict()\n",
        "#         weights['weight_hh_l0'] = torch.randn(hidden_size, hidden_size) * 1/np.sqrt(hidden_size)\n",
        "#         self.rnn.load_state_dict(weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "input_size = 2\n",
        "hidden_size = 500\n",
        "output_size = 2\n",
        "num_layers = 1\n",
        "\n",
        "model = RNN(input_size, hidden_size, output_size, num_layers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcf2707c",
      "metadata": {
        "id": "bcf2707c"
      },
      "source": [
        "Torch initializes the RNN with no particular structure in its recurrent weight matrix. Let's take a look at its eigenvalues to confirm this.\n",
        "\n",
        "<font color=violet>2a. We can access the recurrent weights from our model using the command `model.rnn.state_dict()['weight_hh_l0'].numpy()`. Using this, plot the real and imaginary components of the eigenvalues of the untrained weight matrix. Add a plot of a circle whose radius is the variance of the weights divided by $\\sqrt(N)$, and confirm that this roughly encloses the eigenvalues.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93dbbaf5",
      "metadata": {
        "id": "93dbbaf5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "52978483",
      "metadata": {
        "id": "52978483"
      },
      "source": [
        "Now, run the following code to train your model on the task! The loss will never go completely to zero (and you might not see a clean drop if you look just at every 5th entry), but the network should still get better at the task over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dacb5cd5",
      "metadata": {
        "id": "dacb5cd5"
      },
      "outputs": [],
      "source": [
        "X_train = torch.tensor(X, dtype=torch.float32)\n",
        "y_train = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    permutation = torch.randperm(X_train.size(0))\n",
        "    for i in range(0, X_train.size(0), batch_size):\n",
        "        indices = permutation[i:i+batch_size]\n",
        "        batch_X, batch_y = X_train[indices], y_train[indices]\n",
        "\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b73f995c",
      "metadata": {
        "id": "b73f995c"
      },
      "source": [
        "Finally, run this cell as a sanity check that training worked. Training an RNN is a bit stochastic, so if your network didn't converge on a solution, you can go back and run for more epochs, or run until the loss drops below a target value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ddb140",
      "metadata": {
        "id": "63ddb140"
      },
      "outputs": [],
      "source": [
        "# Generate a test dataset\n",
        "X_test, y_test = generate_sine_cosine_data(seq_length, 100)\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(X_test).numpy()\n",
        "\n",
        "# Plot the predictions\n",
        "trial = 10\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(y_test[trial, :, 0], label='Actual Sine')\n",
        "plt.plot(predictions[trial, :, 0], label='Predicted Sine')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(y_test[trial, :, 1], label='Actual Cosine')\n",
        "plt.plot(predictions[trial, :, 1], label='Predicted Cosine')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6556af9",
      "metadata": {
        "id": "f6556af9"
      },
      "source": [
        "<font color=violet>2b. What did training do to our recurrent weight matrix? To find out, plot the real and imaginary components of the eigenvalues of the trained weight matrix; on the same plot, add the eigenvalues of the original untrained model and the circle you computed previously. What has changed? Which of these changes are most relevant to the task this network is trying to perform?</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c36f176",
      "metadata": {
        "id": "7c36f176"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9aa5c3cf",
      "metadata": {
        "id": "9aa5c3cf"
      },
      "source": [
        "<font color=violet>2c. Now, let's use our simulation code from problem 1 to look at the effect of this modified weight matrix on the network's dynamics. Modify your original `simulate_RNN` function to now take $J$ as an optional input, and pass it the weight matrix of the untrained or trained model. Drive both versions with 2d Gaussian white noise input and again look at the autocorrelation. Can you see a change in the dynamics of the trained network?</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b91af30a",
      "metadata": {
        "id": "b91af30a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}