{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucas-pinto/NUIN443/blob/main/problemSets/NUIN443_ps1_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f68602f9-aca1-479d-a257-dc40b1fd60ad",
      "metadata": {
        "id": "f68602f9-aca1-479d-a257-dc40b1fd60ad"
      },
      "source": [
        "# PROBLEM SET 1: REGRESSION #\n",
        "\n",
        "Here we will review some of the key concepts from the regression class. <br>\n",
        "We will first play with some toy data, using custom written algorithms to fit it with OLS, least mean squares, or ridge. <br>\n",
        "We will then do logistic regression on some real behavioral data from the Pinto Lab. Have fun! <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9c79ecf-75d5-492d-9ffc-a2553ca593d9",
      "metadata": {
        "id": "f9c79ecf-75d5-492d-9ffc-a2553ca593d9"
      },
      "outputs": [],
      "source": [
        "# IMPORT SOME LIBRARIES\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from scipy import stats # for z scoring\n",
        "from sklearn.model_selection import RepeatedKFold # for cross val in problem 4\n",
        "from sklearn.linear_model import LogisticRegression # for problem 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da0e6f9-347b-4b29-84b2-e37325a2b272",
      "metadata": {
        "id": "3da0e6f9-347b-4b29-84b2-e37325a2b272"
      },
      "outputs": [],
      "source": [
        "# DOWNLOAD DATA\n",
        "!wget -nc https://www.dropbox.com/scl/fi/u430bflj0wa8u40bip539/nuin443_ols_data.pickle?rlkey=x6gggiaxx3kwcn20d5lzkf2dj&dl=0\n",
        "!wget -nc https://www.dropbox.com/scl/fi/khlt5gezj3cwqscmfg3u7/nuin443_ridge_data.pickle?rlkey=63tersdczwyln9brx0pm06rs2&dl=0\n",
        "!wget -nc https://www.dropbox.com/scl/fi/0ohjs6xxlgfx3qumfzpvi/nuin443_towers_df.pickle?rlkey=5td9e8dl0zwy1qgdpb8i2cgqb&dl=0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47e7f0e5-97a9-4679-9f2d-98f5ef6ec712",
      "metadata": {
        "id": "47e7f0e5-97a9-4679-9f2d-98f5ef6ec712"
      },
      "source": [
        "## Problem 1: OLS ##\n",
        "\n",
        "**1a**. Load the data. \\\\(X\\\\) is the predictor matrix and \\\\(y\\\\) is the response vector. <br>\n",
        "**1b**. Is this a good candidate for OLS? Check that the correlation matrix \\\\(X^TX\\\\) is not singular.  <br>\n",
        "**1c**. Write your own OLS function using linear algebra.  <br>\n",
        "**1d**. Fit the data and print the weight vector, including the intercept term.  <br>\n",
        "**1e**. Write functions to compute the coefficient of determination \\\\(R^2\\\\) and the squared correlation coefficient between data and prediction, \\\\(r^2\\\\).  <br>\n",
        "**1f**. Use your functions to assess the goodness of fit using both metrics. How do they compare? What does that say about the data?  <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dba9c7ed-5ec2-40bd-88a2-e4b0edac4f83",
      "metadata": {
        "id": "dba9c7ed-5ec2-40bd-88a2-e4b0edac4f83"
      },
      "outputs": [],
      "source": [
        "# 1a. Load data\n",
        "filename    = 'nuin443_ols_data.pickle?rlkey=x6gggiaxx3kwcn20d5lzkf2dj'\n",
        "with open(filename, 'rb') as handle:\n",
        "    [X,y] = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcfbc677-b0d4-43e0-8db8-3ae8a4ac48c5",
      "metadata": {
        "id": "fcfbc677-b0d4-43e0-8db8-3ae8a4ac48c5"
      },
      "outputs": [],
      "source": [
        "# 1b. Is X non-singular?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d980e1-f50f-48a7-828c-cd507ba6b78d",
      "metadata": {
        "id": "69d980e1-f50f-48a7-828c-cd507ba6b78d"
      },
      "outputs": [],
      "source": [
        "# 1c. Define your OLS function (fill this in)\n",
        "\n",
        "def OLS(X,y):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07bfea17-10f7-44f0-946c-fdfe2798a743",
      "metadata": {
        "id": "07bfea17-10f7-44f0-946c-fdfe2798a743"
      },
      "outputs": [],
      "source": [
        "# 1d. Use your OLS function to find the weight vector w (remeber to add intercept!). Print result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5982709c-e3b3-4dc0-a94c-1c2acea9434d",
      "metadata": {
        "id": "5982709c-e3b3-4dc0-a94c-1c2acea9434d"
      },
      "outputs": [],
      "source": [
        "# 1e. Define goodness-of-fit functions:\n",
        "\n",
        "# Coefficient of determination (fill this in)\n",
        "def fit_R2(y,y_hat):\n",
        "\n",
        "\n",
        "# Squared correlation coefficient (fill this in)\n",
        "def fit_r2(y,y_hat):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90ecb6cb-9135-42ce-94c0-9ee61529e911",
      "metadata": {
        "id": "90ecb6cb-9135-42ce-94c0-9ee61529e911"
      },
      "outputs": [],
      "source": [
        "# 1f. Use the functions above to assess goodness of fit, print results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29607ab4-048d-41bd-8893-e6a2ae049a05",
      "metadata": {
        "id": "29607ab4-048d-41bd-8893-e6a2ae049a05"
      },
      "outputs": [],
      "source": [
        "# 1f. How do they compare? What does that say about the data? (answer as code comment)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "188e8d99-8bcd-4dc6-81de-1a138cd01bfd",
      "metadata": {
        "id": "188e8d99-8bcd-4dc6-81de-1a138cd01bfd"
      },
      "source": [
        "## Problem 2: LMS ##\n",
        "**2a**. Now write your own least mean squares algorithm (gradient descent) and <br>\n",
        "**2b**. fit the same data. Print \\\\(w\\\\)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de146f82-6533-4929-8d07-786fe76a35ab",
      "metadata": {
        "id": "de146f82-6533-4929-8d07-786fe76a35ab"
      },
      "outputs": [],
      "source": [
        "# 2a. Write your LMS algorithm.\n",
        "# I'm suggesting some defaults for the hyperparameters\n",
        "# (alpha is learning rate, tolerance is the amount of change in the gradient that makes you stop fitting)\n",
        "\n",
        "def LMS(X,y,alpha=.001,maxiter=500,tolerance=1e-5):\n",
        "\n",
        "    # initialize random weights\n",
        "    w  = np.random.normal(size=np.size(X,axis=1))\n",
        "    Xt = np.transpose(X)\n",
        "\n",
        "    for i in range(maxiter):\n",
        "        # compute gradient epsilon (fill this in)\n",
        "\n",
        "        if sum(abs(epsilon)) < tolerance:\n",
        "            break\n",
        "        else:\n",
        "            # update weight (fill this in)\n",
        "\n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93ff7373-e757-4199-a538-9c7358fe5228",
      "metadata": {
        "id": "93ff7373-e757-4199-a538-9c7358fe5228"
      },
      "outputs": [],
      "source": [
        "# 2b. Fit the same data with your LMS function and print w\n",
        "\n",
        "Xs       = np.ones((500,5))\n",
        "Xs[:,1:] = X\n",
        "w        = LMS(Xs,y)\n",
        "print(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c0945c9-5726-4539-96c3-559958fdf4d9",
      "metadata": {
        "id": "0c0945c9-5726-4539-96c3-559958fdf4d9"
      },
      "source": [
        "### Problem 3: ridge regression ##\n",
        "\n",
        "**3a**. Load the new dataset \\\\(X_r\\\\) and \\\\(y_r\\\\). This time I made two of the variables correlated <br>\n",
        "**3b**. Compute and plot the correlation matrix \\\\(X_r^TX_r\\\\) to figure out which are the correlated columns of \\\\(X_r\\\\) <br>\n",
        "**3c**. Write your own function to do ridge regression using the analytical (linear algebra) solution <br>\n",
        "**3d**. Use your function to fit the data, varying \\\\(\\lambda\\\\) systematically between 0 and 100 <br>\n",
        "**3e**. Plot the weight for each parameter as a function of \\\\(\\lambda\\\\). What happens to weights for correlated vs. uncorrelated variables?  <br>\n",
        "(note: you should use cross-validation irl to determine the best \\\\(\\lambda\\\\) for your model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6951aa18-4869-4b63-9401-4b928ca2900f",
      "metadata": {
        "id": "6951aa18-4869-4b63-9401-4b928ca2900f"
      },
      "outputs": [],
      "source": [
        "# 3a. load data\n",
        "filename    = 'nuin443_ridge_data.pickle?rlkey=63tersdczwyln9brx0pm06rs2'\n",
        "with open(filename, 'rb') as handle:\n",
        "    [Xr,yr] = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "042001ef-1600-4276-8529-067ba8ecb36c",
      "metadata": {
        "id": "042001ef-1600-4276-8529-067ba8ecb36c"
      },
      "outputs": [],
      "source": [
        "# 3b. Compute and plot X^TX to figure out which are the correlated variables\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ae9132b-59ae-4d5d-8e16-630f5b87522e",
      "metadata": {
        "id": "7ae9132b-59ae-4d5d-8e16-630f5b87522e"
      },
      "outputs": [],
      "source": [
        "# 3c. Write your ridge function (fill this in)\n",
        "\n",
        "def ridge(X,y,lam):\n",
        "\n",
        "\n",
        "    return wr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53515b94-625b-48ca-a959-ee2677e94503",
      "metadata": {
        "id": "53515b94-625b-48ca-a959-ee2677e94503"
      },
      "outputs": [],
      "source": [
        "# 3d. Fit the model with different values of lambda in the [0 100] interval\n",
        "# Hint: remember to zscore/center X and y!\n",
        "\n",
        "wr_all    = np.zeros((100,4))\n",
        "lams      = np.linspace(0,100,100)\n",
        "Xr_norm   = stats.zscore(Xr)\n",
        "ys        = yr-np.mean(yr)\n",
        "\n",
        "for i in range(len(lams)):\n",
        "    wr_all[i,:] = ridge(Xr_norm,ys,lams[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "628fa86b-07f6-4dc2-b59f-5c57a2cefa2f",
      "metadata": {
        "id": "628fa86b-07f6-4dc2-b59f-5c57a2cefa2f"
      },
      "outputs": [],
      "source": [
        "# 3d. Plot each weight as a function of lambda (on the same axis)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(lams,wr_all[:,0])\n",
        "plt.plot(lams,wr_all[:,1])\n",
        "plt.plot(lams,wr_all[:,2])\n",
        "plt.plot(lams,wr_all[:,3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0572fc8-9e95-4703-863d-ea0941c1cd5b",
      "metadata": {
        "id": "a0572fc8-9e95-4703-863d-ea0941c1cd5b"
      },
      "outputs": [],
      "source": [
        "# 3d. What happens to weights for correlated vs. uncorrelated variables? (answer as a comment)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa68bf51-7c37-4710-ae29-1d914152d7bc",
      "metadata": {
        "id": "fa68bf51-7c37-4710-ae29-1d914152d7bc"
      },
      "source": [
        "## Problem 4: Bernoulli GLM (logistic regression) of behavioral choice ##\n",
        "\n",
        "Now you get to play with some Pinto lab data! :-D <br>\n",
        "This is choice data from the accumulating-towers task (Pinto*, Koay* et al, 2018, Front Behav Neurosci). Please refer to the paper if you're curious about the details. <br>\n",
        "What you need to know for this problem is that mice navigate a virtual T maze, and experience pulsed visual stimuli (towers), which occur randomly (following Poisson statistics) on the right (R) and left (L) sides of the maze. This means that, on different trials, there are different number of R and L towers. The mice get rewarded for turning into the side arm corresponding to the side where they saw the most towers. Indeed, we see that their performance is clearly modulated by the amount of sensory evidence, which we will parametrize as \\\\( \\Delta_{towers} = num. R \\ – \\ num. L\\\\). The thing that makes this problem fun, though, is that the ideal strategy is to just pay attention to the towers (since reward is deterministic), but mice often go through bouts of non-stimulus-driven startegies, like exploration (\"let me check out the other arm just in case\"), perseveration (\"whatever, I'm just picking left no matter what\"), reward-based strategies (win-stay, win-shift). What's in common about these non-stimulus-driven strategies is that they all depend on the history of previous rewards and behavioral choices. We will use regression to estimate the relative impact of those strategies. <br>\n",
        "\n",
        "**4a**. Load the data (see comments below for an explanation of the format) <br>\n",
        "**4b**. Build \\\\(X\\\\) and \\\\(y\\\\) for three GLMs (some tips below): <br>\n",
        "1) Just \\\\(\\Delta_{towers}\\\\) <br>\n",
        "2) \\\\(\\Delta_{towers}\\\\) and two history terms going one trial back: \\\\(choice\\\\) and \\\\(choice \\times reward\\\\). The latter is an interaction term (literally modeled by their multiplication), which captures what happens after a left or a a right choice is rewarded (or not) <br>\n",
        "3) Same as 2, but with history predictors going up to 5 trials back <br>\n",
        "\n",
        "**4c**. Set up 3 rounds of 10-fold cross-validation for each model, using the same seed, fit without regularization, and quantify goodness of fit on held-out data by computing the proportion of correctly classified choices (i.e., the accuracy). <br>\n",
        "**4d**. Plot the accuracy across each cross-validation run to see which is the best model. <br>\n",
        "**4e**. Plot the weights for the best model (average and std across cross-validation runs). Which feature is most predictive of performance? What do the signs of the weights mean? <br>\n",
        "**4f**. For the best model only, refit with L1 regularization, and print the average weights. What are the differences in the weights, if any? What does that say about the data? <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a775271-4953-4299-b11b-ec609a73831b",
      "metadata": {
        "id": "1a775271-4953-4299-b11b-ec609a73831b",
        "outputId": "7a40b816-4d81-4337-ea8a-fc0335e3f71a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject_fullname</th>\n",
              "      <th>session_date</th>\n",
              "      <th>session_number</th>\n",
              "      <th>trial_id</th>\n",
              "      <th>trial_type</th>\n",
              "      <th>choice</th>\n",
              "      <th>num_towers_right</th>\n",
              "      <th>num_towers_left</th>\n",
              "      <th>num_towers_delta</th>\n",
              "      <th>trial_duration</th>\n",
              "      <th>block_performance</th>\n",
              "      <th>is_first_trial_of_block</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>jlt6316_Chomp</td>\n",
              "      <td>2023-06-08</td>\n",
              "      <td>1</td>\n",
              "      <td>118</td>\n",
              "      <td>R</td>\n",
              "      <td>R</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>8.95949</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>jlt6316_Chomp</td>\n",
              "      <td>2023-06-08</td>\n",
              "      <td>1</td>\n",
              "      <td>119</td>\n",
              "      <td>R</td>\n",
              "      <td>R</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>8.62593</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>jlt6316_Chomp</td>\n",
              "      <td>2023-06-08</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>L</td>\n",
              "      <td>R</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>-2</td>\n",
              "      <td>8.71748</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>jlt6316_Chomp</td>\n",
              "      <td>2023-06-08</td>\n",
              "      <td>1</td>\n",
              "      <td>121</td>\n",
              "      <td>L</td>\n",
              "      <td>R</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>-6</td>\n",
              "      <td>8.51737</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>jlt6316_Chomp</td>\n",
              "      <td>2023-06-08</td>\n",
              "      <td>1</td>\n",
              "      <td>122</td>\n",
              "      <td>L</td>\n",
              "      <td>R</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>-5</td>\n",
              "      <td>8.30914</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77443</th>\n",
              "      <td>jlt6316_JsChileanGrouper</td>\n",
              "      <td>2024-02-06</td>\n",
              "      <td>1</td>\n",
              "      <td>87</td>\n",
              "      <td>R</td>\n",
              "      <td>L</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>7.39246</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77444</th>\n",
              "      <td>jlt6316_JsChileanGrouper</td>\n",
              "      <td>2024-02-06</td>\n",
              "      <td>1</td>\n",
              "      <td>88</td>\n",
              "      <td>L</td>\n",
              "      <td>L</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>-5</td>\n",
              "      <td>7.20059</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77445</th>\n",
              "      <td>jlt6316_JsChileanGrouper</td>\n",
              "      <td>2024-02-06</td>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>L</td>\n",
              "      <td>L</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-4</td>\n",
              "      <td>7.31712</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77446</th>\n",
              "      <td>jlt6316_JsChileanGrouper</td>\n",
              "      <td>2024-02-06</td>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>L</td>\n",
              "      <td>L</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>-2</td>\n",
              "      <td>7.95918</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77447</th>\n",
              "      <td>jlt6316_JsChileanGrouper</td>\n",
              "      <td>2024-02-06</td>\n",
              "      <td>1</td>\n",
              "      <td>91</td>\n",
              "      <td>R</td>\n",
              "      <td>L</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>7.42557</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>33421 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               subject_fullname session_date  session_number  trial_id  \\\n",
              "117               jlt6316_Chomp   2023-06-08               1       118   \n",
              "118               jlt6316_Chomp   2023-06-08               1       119   \n",
              "119               jlt6316_Chomp   2023-06-08               1       120   \n",
              "120               jlt6316_Chomp   2023-06-08               1       121   \n",
              "121               jlt6316_Chomp   2023-06-08               1       122   \n",
              "...                         ...          ...             ...       ...   \n",
              "77443  jlt6316_JsChileanGrouper   2024-02-06               1        87   \n",
              "77444  jlt6316_JsChileanGrouper   2024-02-06               1        88   \n",
              "77445  jlt6316_JsChileanGrouper   2024-02-06               1        89   \n",
              "77446  jlt6316_JsChileanGrouper   2024-02-06               1        90   \n",
              "77447  jlt6316_JsChileanGrouper   2024-02-06               1        91   \n",
              "\n",
              "      trial_type choice num_towers_right num_towers_left num_towers_delta  \\\n",
              "117            R      R                9               1                8   \n",
              "118            R      R                9               0                9   \n",
              "119            L      R                2               4               -2   \n",
              "120            L      R                4              10               -6   \n",
              "121            L      R                2               7               -5   \n",
              "...          ...    ...              ...             ...              ...   \n",
              "77443          R      L               10               1                9   \n",
              "77444          L      L                4               9               -5   \n",
              "77445          L      L                2               6               -4   \n",
              "77446          L      L                3               5               -2   \n",
              "77447          R      L                5               1                4   \n",
              "\n",
              "       trial_duration  block_performance is_first_trial_of_block  \n",
              "117           8.95949                0.6                       1  \n",
              "118           8.62593                0.6                       0  \n",
              "119           8.71748                0.6                       0  \n",
              "120           8.51737                0.6                       0  \n",
              "121           8.30914                0.6                       0  \n",
              "...               ...                ...                     ...  \n",
              "77443         7.39246                0.5                       0  \n",
              "77444         7.20059                0.5                       0  \n",
              "77445         7.31712                0.5                       0  \n",
              "77446         7.95918                0.5                       0  \n",
              "77447         7.42557                0.5                       0  \n",
              "\n",
              "[33421 rows x 12 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 4a. Load the data\n",
        "# This is a dataframe with some pre-selected data from 4 mice performing the task,\n",
        "# where each row corresponds to a behavioral trial (n=33421). The columns you will need for X are:\n",
        "#       trial_type: whether this is a trial where the reward is on the R or L\n",
        "#       choice: which way the mouse turned\n",
        "#       num_towers_delta: #R – #L towers\n",
        "#       is_first_trial_of_block: we will need this to determine trial history (more on this below)\n",
        "\n",
        "filename    = 'nuin443_towers_df.pickle?rlkey=5td9e8dl0zwy1qgdpb8i2cgqb'\n",
        "with open(filename, 'rb') as handle:\n",
        "    towers_trial_df = pickle.load(handle)\n",
        "\n",
        "towers_trial_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a403ad5c-b12f-40e2-8c46-d661370b3928",
      "metadata": {
        "id": "a403ad5c-b12f-40e2-8c46-d661370b3928"
      },
      "outputs": [],
      "source": [
        "# 4b. set up y and X's.\n",
        "# y should be a vector of 1's and 0's, where 1 == right and 0 == left (Bernoulli variable)\n",
        "# set up the 3 models above as X_1, X_2, and X_3\n",
        "# for setting up trial history terms, I recommend parametrizing choice as -1 and 1 instead.\n",
        "# This is because of the interaction term should have three behaviors: didn't get reward (0),\n",
        "# got a reward on the left (-1), got a reward on the right (1). This different parametrization\n",
        "# doesn't matter because you will z-score X... right?\n",
        "# also, you can tell if there was a reward by comparing choice and trial type\n",
        "\n",
        "# build y\n",
        "ch_str     = towers_trial_df['choice'].to_numpy()\n",
        "num_trials = len(ch_str)\n",
        "y          = np.zeros(num_trials)\n",
        "y[ch_str=='R'] = 1\n",
        "\n",
        "# build the relevant vectors\n",
        "choice              = np.zeros(num_trials)\n",
        "choice[ch_str=='R'] = 1\n",
        "choice[ch_str=='L'] = -1\n",
        "\n",
        "tt_str                  = towers_trial_df['trial_type'].to_numpy()\n",
        "trial_type              = np.zeros(num_trials)\n",
        "trial_type[tt_str=='R'] = 1\n",
        "trial_type[tt_str=='L'] = -1\n",
        "\n",
        "reward        = (choice == trial_type).astype(float)\n",
        "reward_choice = reward*choice\n",
        "\n",
        "delta_towers  = (towers_trial_df['num_towers_delta'].to_numpy()).astype(float)\n",
        "\n",
        "# build X_1, X_2, X_3 (fill this in) (remember the lags strategy from the lecture)\n",
        "\n",
        "\n",
        "# Now there is a crucial step.\n",
        "# This is actually in the context on a task switching paradigm, s.t. not\n",
        "# every trial is contiguous. And also, we are concatenating sessions.\n",
        "# We can tell that by the column is_first_trial_of_block, a boolean\n",
        "# that equals 1 (True) for trials that do not have history.\n",
        "# Also, because model X_3 goes 5 trials back, the first 4 trials\n",
        "# in the block do not have history and need to be deleted from the rows of X's and y.\n",
        "# This is only necessary for X_3, but we want exactly the same trials in every\n",
        "# model for fair comparison.\n",
        "# The final trial count should be 19,720\n",
        "# (fill this in)\n",
        "\n",
        "\n",
        "# z score\n",
        "X_1   = stats.zscore(X_1)\n",
        "X_2   = stats.zscore(X_2)\n",
        "X_3   = stats.zscore(X_3)\n",
        "\n",
        "# put in a list for easy iteration\n",
        "Xlist = [X_1,X_2,X_3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0777b428-4059-45bd-b27e-8e62e73a8e16",
      "metadata": {
        "id": "0777b428-4059-45bd-b27e-8e62e73a8e16"
      },
      "outputs": [],
      "source": [
        "# 4c. Set up 3 rounds of 10-fold cross-validation for each model, using the same seed\n",
        "# fit without regularization and quantify goodness of fit on held-out data\n",
        "# by computing the proportion of correctly classified choices (i.e., the accuracy).\n",
        "\n",
        "# cross-validation set up\n",
        "num_fold    = 10\n",
        "num_repeats = 3\n",
        "kf          = RepeatedKFold(n_splits=num_fold,n_repeats=num_repeats,random_state=42)\n",
        "\n",
        "# set up model\n",
        "model_noreg = LogisticRegression(penalty='none',max_iter=500,solver='saga')\n",
        "\n",
        "# cross-validate\n",
        "fits_noreg  = [[],[],[]]\n",
        "acc_noreg   = [[],[],[]]\n",
        "coef_noreg  = [[],[],[]]\n",
        "\n",
        "for iModel in range(len(Xlist)):\n",
        "    print(f\"Model {iModel}...\")\n",
        "    for iXval, (train_index, test_index) in enumerate(kf.split(Xlist[iModel])):\n",
        "        # (fill this in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d0ef6a9-952a-4d9f-baa7-fab34599ba43",
      "metadata": {
        "id": "1d0ef6a9-952a-4d9f-baa7-fab34599ba43"
      },
      "outputs": [],
      "source": [
        "# 4d. Plot the accuracy across each cross-validation run to see which is the best model,\n",
        "# connecting the corresponding cross-val runs with lines\n",
        "\n",
        "acc = np.array(acc_noreg)\n",
        "acc_mean = np.mean(acc,axis=1)\n",
        "acc_std = np.std(acc,axis=1,ddof=1)\n",
        "plt.figure()\n",
        "for i in range(np.size(acc,axis=1)):\n",
        "    plt.plot(np.arange(3),acc[:,i],c='grey')\n",
        "plt.scatter(np.arange(3),acc_mean,c='k')\n",
        "plt.errorbar(np.arange(3),acc_mean,acc_std,c='k')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a909abbe-9291-4ea9-a708-f8144aacecbd",
      "metadata": {
        "id": "a909abbe-9291-4ea9-a708-f8144aacecbd"
      },
      "outputs": [],
      "source": [
        "# 4e. Plot weights of best model (average and std across cross-validation runs)\n",
        "best_idx = # fill in with index of the best model\n",
        "coeffs   = np.array(coef_noreg[best_idx])\n",
        "coeffs_m = np.mean(coeffs,axis=0)\n",
        "coeffs_s = np.std(coeffs,axis=0,ddof=1)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(0,coeffs_m[0],c='k')\n",
        "plt.errorbar(0,coeffs_m[0],coeffs_s[0],c='k')\n",
        "thisx = np.arange(1,1+num_hist)\n",
        "plt.scatter(thisx+1,coeffs_m[thisx],c='k')\n",
        "plt.errorbar(thisx+1,coeffs_m[thisx],coeffs_s[thisx],c='k')\n",
        "thisx = np.arange(1+num_hist,len(coeffs_s))\n",
        "plt.scatter(thisx+2,coeffs_m[thisx],c='k')\n",
        "plt.errorbar(thisx+2,coeffs_m[thisx],coeffs_s[thisx],c='k')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fda3257-2721-45e4-bf1b-d77f26bdec42",
      "metadata": {
        "id": "1fda3257-2721-45e4-bf1b-d77f26bdec42"
      },
      "outputs": [],
      "source": [
        "# 4e. Which feature is most predictive of performance? What do the signs of the weights mean?\n",
        "# (answer as a comment)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61b113e4-4502-4dab-bec6-17373c864686",
      "metadata": {
        "id": "61b113e4-4502-4dab-bec6-17373c864686"
      },
      "outputs": [],
      "source": [
        "# 4f. For the best model only, refit with L1 regularization, and print the average weights.\n",
        "model_l1 = LogisticRegression(penalty='l1',max_iter=500,solver='saga')\n",
        "fits_l1  = [[],[],[]]\n",
        "acc_l1   = [[],[],[]]\n",
        "coef_l1  = [[],[],[]]\n",
        "\n",
        "iModel = best_idx\n",
        "print(f\"Model {iModel}...\")\n",
        "for iXval, (train_index, test_index) in enumerate(kf.split(Xlist[iModel])):\n",
        "    # fill in (it will be basically the same as 4c)\n",
        "\n",
        "# compare average coefficients with and without regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5af277a1-ecc2-4ef5-8ed7-7cb3a9e1e6ff",
      "metadata": {
        "id": "5af277a1-ecc2-4ef5-8ed7-7cb3a9e1e6ff"
      },
      "outputs": [],
      "source": [
        "# 4f. What are the differences in the weights, if any? What does that say about the data?\n",
        "# (answer in a comment)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43de4efe-1cae-4b5a-b1e6-3fc8f2529b82",
      "metadata": {
        "id": "43de4efe-1cae-4b5a-b1e6-3fc8f2529b82"
      },
      "source": [
        "## Bonus problem: Mixed-effects models ##\n",
        "\n",
        "**This is for the aficionados and will not be graded** <br>\n",
        "The dataset above has the classic clustered design in neuroscience: data is being combined across mice, and for each mice across sessions. <br>\n",
        "Fit a mixed-effects logistic regression model using random intercepts for mice (column **subject_fullname**) and sessions (column **session_date**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7416e6d9-fed6-4444-85c7-a8e8419c52c0",
      "metadata": {
        "id": "7416e6d9-fed6-4444-85c7-a8e8419c52c0"
      },
      "outputs": [],
      "source": [
        "# The pymer4 package is great for this: https://eshinjolly.com/pymer4/\n",
        "# If I recall correctly, it needs to have R installed\n",
        "\n",
        "from   pymer4.models import Lmer"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}